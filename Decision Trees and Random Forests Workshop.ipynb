{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree -- Theory**\n",
    "\n",
    "A decision tree is a predictive model that can perform classification and regression tasks. It takes as input an observation vector $\\vec{x} = (x_1, \\dots, x_n)$ of variables and makes a prediction $y$ based on a series of binary decisions.\n",
    "\n",
    "The following image shows a diagram of a decision tree built to model whether a passenger on the Titanic survived the crash. Our observation has three features, one discrete, gender, and two continuous, age and number of siblings or spouses onboard (`sibsp`). The label on each leaf shows the decision that the tree makes about the passenger, and below the leaf is written the true probability that such a passenger survived and the percentage of passengers that are represented by each leaf of the tree. \n",
    "\n",
    "![Decision tree diagram](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "\n",
    "We can train such a model of our own given a set of observations and targets $\\{(\\vec{x}, y)\\}$. The general idea for the algorithms that do this training is recursive: We pick the feature in the observation most correlated to our target and split the data into two subsets depending on their value for that feature. This process recurses on the two smaller subsets until the decision tree reaches a prespecified depth or accuracy.\n",
    "\n",
    "One way of picking the most decisive feature for a subset involves calculating the Shannon entropy, $H(Y)$. This is a function on a distribution $p(y)$ which is higher the more 'random' the variable is. For each feature $X_i$ we can then calculate the conditional entropy of $Y$, $H(Y|X_i)$. The conditional entropy tells us how random our variable is given that we know something, in this case $X_i$, about it. We can use these quantities to tell us the **information gain** associated with each variable, by calculating $IG = H(Y) - H(Y|X_i)$. The splits with the most information gain are the best candidates for a node of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest -- Theory**\n",
    "\n",
    "As the name implies, a random forest is built from an **ensemble** of decision trees. While this method improves the accuracy of the classifier/regressor, the results are clearly less interpretable than they are in a single decision tree.\n",
    "\n",
    "So how do we create more than one tree? The naive approach would be to repeat the above algorithm for decision trees multiple times, but this would end up creating highly-correlated trees. Instead, we can **bag** the data: for each tree, choose $n$ samples from the training set (*with* replacement).\n",
    "\n",
    "Now, if any features are strong predictors across the data set, they will likely be chosen earlier in the learning process for multiple trees, which may cause the trees to become correlated. To remedy this, at each step (node) of the learning process for the decision tree, consider only $d$ distinct features, and select the best feature to split the node.\n",
    "\n",
    "To use this model for evaluation, we can take an aggregate result of the decision trees. For a classification prediction, this involves a simple majority vote of the trees' predicted values. For a regression problem, the model takes the average of all individual regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will work on implementing the Decision Tree and Random Forest architectures. The first step is importing the packages and methods we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is making the data. We will use two datasets for this project. ___ To make the data easier to work with, we will convert the csv files into pandas dataframes. We will also seperate the target variables (i.e. the y values) into seperate objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DSWRF_SFC_0000</th>\n",
       "      <th>DSWRF_SFC_0001</th>\n",
       "      <th>DSWRF_SFC_0002</th>\n",
       "      <th>DSWRF_SFC_0003</th>\n",
       "      <th>DSWRF_SFC_0004</th>\n",
       "      <th>DSWRF_SFC_0005</th>\n",
       "      <th>DSWRF_SFC_0006</th>\n",
       "      <th>DSWRF_SFC_0007</th>\n",
       "      <th>DSWRF_SFC_0008</th>\n",
       "      <th>DSWRF_SFC_0009</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_SFC_0019</th>\n",
       "      <th>VIS_SFC_0020</th>\n",
       "      <th>VIS_SFC_0021</th>\n",
       "      <th>VIS_SFC_0022</th>\n",
       "      <th>VIS_SFC_0023</th>\n",
       "      <th>VIS_SFC_0024</th>\n",
       "      <th>time_of_day_cos</th>\n",
       "      <th>time_of_day_sin</th>\n",
       "      <th>time_of_year_cos</th>\n",
       "      <th>time_of_year_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7.750</td>\n",
       "      <td>18.25</td>\n",
       "      <td>...</td>\n",
       "      <td>426.895569</td>\n",
       "      <td>227.899689</td>\n",
       "      <td>228.550201</td>\n",
       "      <td>428.331696</td>\n",
       "      <td>227.286820</td>\n",
       "      <td>224.271637</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.977848</td>\n",
       "      <td>0.209315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.5</td>\n",
       "      <td>63.875</td>\n",
       "      <td>90.750</td>\n",
       "      <td>17.875</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>7824.965820</td>\n",
       "      <td>4825.635742</td>\n",
       "      <td>2627.959229</td>\n",
       "      <td>2628.863281</td>\n",
       "      <td>4828.833008</td>\n",
       "      <td>9027.083984</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.976011</td>\n",
       "      <td>0.217723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>24226.927730</td>\n",
       "      <td>24223.308590</td>\n",
       "      <td>14824.718750</td>\n",
       "      <td>20222.730470</td>\n",
       "      <td>16624.769530</td>\n",
       "      <td>19425.916020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.975065</td>\n",
       "      <td>0.221922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>60.125</td>\n",
       "      <td>50.25</td>\n",
       "      <td>...</td>\n",
       "      <td>5625.299316</td>\n",
       "      <td>8026.288086</td>\n",
       "      <td>20024.955080</td>\n",
       "      <td>6023.587402</td>\n",
       "      <td>24223.126950</td>\n",
       "      <td>24222.828130</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.226116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.625</td>\n",
       "      <td>105.625</td>\n",
       "      <td>55.375</td>\n",
       "      <td>140.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>80.125</td>\n",
       "      <td>44.50</td>\n",
       "      <td>...</td>\n",
       "      <td>23823.607420</td>\n",
       "      <td>24223.201170</td>\n",
       "      <td>10823.557620</td>\n",
       "      <td>17022.439450</td>\n",
       "      <td>5824.313965</td>\n",
       "      <td>24222.529300</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973118</td>\n",
       "      <td>0.230306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DSWRF_SFC_0000  DSWRF_SFC_0001  DSWRF_SFC_0002  DSWRF_SFC_0003  \\\n",
       "0             0.0           0.000           0.000           0.000   \n",
       "1           106.5          63.875          90.750          17.875   \n",
       "2             0.0           0.000           0.000           0.000   \n",
       "3             0.0           0.000           0.000           0.000   \n",
       "4             0.0           8.625         105.625          55.375   \n",
       "\n",
       "   DSWRF_SFC_0004  DSWRF_SFC_0005  DSWRF_SFC_0006  DSWRF_SFC_0007  \\\n",
       "0            0.00             0.0            0.00            1.75   \n",
       "1           10.00             0.0            0.00            0.00   \n",
       "2            0.00             0.0            0.00            0.00   \n",
       "3            0.00             0.0            0.00            6.50   \n",
       "4          140.75           100.0           77.25           56.25   \n",
       "\n",
       "   DSWRF_SFC_0008  DSWRF_SFC_0009  ...  VIS_SFC_0019  VIS_SFC_0020  \\\n",
       "0           7.750           18.25  ...    426.895569    227.899689   \n",
       "1           0.000            0.00  ...   7824.965820   4825.635742   \n",
       "2           0.000            0.00  ...  24226.927730  24223.308590   \n",
       "3          60.125           50.25  ...   5625.299316   8026.288086   \n",
       "4          80.125           44.50  ...  23823.607420  24223.201170   \n",
       "\n",
       "   VIS_SFC_0021  VIS_SFC_0022  VIS_SFC_0023  VIS_SFC_0024  time_of_day_cos  \\\n",
       "0    228.550201    428.331696    227.286820    224.271637                0   \n",
       "1   2627.959229   2628.863281   4828.833008   9027.083984                0   \n",
       "2  14824.718750  20222.730470  16624.769530  19425.916020                1   \n",
       "3  20024.955080   6023.587402  24223.126950  24222.828130                0   \n",
       "4  10823.557620  17022.439450   5824.313965  24222.529300               -1   \n",
       "\n",
       "   time_of_day_sin  time_of_year_cos  time_of_year_sin  \n",
       "0                1          0.977848          0.209315  \n",
       "1               -1          0.976011          0.217723  \n",
       "2                0          0.975065          0.221922  \n",
       "3                1          0.974100          0.226116  \n",
       "4                0          0.973118          0.230306  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process CSV files into dataframes\n",
    "cancer_df = pd.read_csv('data/breast_cancer.csv')\n",
    "cancer_df.pop('ID')\n",
    "solar_df = pd.read_csv('data/solar.csv')\n",
    "\n",
    "# Make target objects\n",
    "cancer_y = cancer_df.pop('diagnosis')\n",
    "solar_y = solar_df.pop('SOLARRADIATION_0003')\n",
    "solar_df = solar_df.iloc[:,1:]\n",
    "\n",
    "# Print results\n",
    "cancer_df.head()\n",
    "solar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another imporant part of data processing is normalizing the data. This usually means putting each data value in the dataframe somewhere between one and zero. Often, this is done by subtracting the observation from the maximum observation and then dividing by the range ($scaled=\\frac{max - unscaled}{max - min}$). We will use a built in sklearn function to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "    x = df.values\n",
    "    scaler = MinMaxScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    return pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the method for making a model. You will see the method `makeModel` takes five parameters: `num_features, model, df, y, clf`. `num_features` is the number of input features to consider, as we will only use the n features containing the highest variance. The model `model` is specifies the predefined sklearn model we will use. `df` is the dataframe we will use, `y` is the corresponding target variable, and `clf` is True if we are performing a classification task and False if we are performing a regression task.  \n",
    "\n",
    "First, we preprocess the data by normalizing, choosing for features with the highest variance, and splitting into training and testing (we use an 80/20 split). Next, we train the model (using sklearn's `.fit` function) and predict results for the testing data. Lastly, we will calculate metrics to measure the success of the predictions on the testing data. If it's a classification task, we will calculate accuracy, recall, and f1 score. If it is a regression task, we will caclulate mean squared error and the $r^2$ score. If you are not sure how any of these scores are calculated, take a minute to research them. Which of these metrics do you think are most important? Also, notice how many tasks we can simply use the built-in sklearn method for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel(num_features, model, df, y, clf):\n",
    "    # Restrict to only features with highest variance + other preprocessing\n",
    "    cols = df.var().sort_values()[(-num_features - 1):].keys()\n",
    "    df = df[df.columns.intersection(cols)]\n",
    "    df = scale(df)\n",
    "    training_x, testing_x, training_y, testing_y = train_test_split(df, y, test_size=0.2)\n",
    "    \n",
    "    # Fit model and predict results\n",
    "    model.fit(training_x, training_y)\n",
    "    pred = model.predict(testing_x)\n",
    "    print(str(model) + \" with \" + str(num_features) + \" features: \")\n",
    "    \n",
    "    if clf:       \n",
    "        # Test for accuracy, recall, and f1 score\n",
    "        accuracy = accuracy_score(testing_y, pred)\n",
    "        recall = recall_score(testing_y, pred, pos_label = 'M') #Double Check? \n",
    "        f1 = f1_score(testing_y, pred, pos_label = 'M')\n",
    "        print (\"\\tAccuracy of \" + str(accuracy))\n",
    "        print (\"\\tRecall of \" + str(recall))\n",
    "        print (\"\\tF1 of \" + str(f1))\n",
    "    else:\n",
    "        # Test for mean squared error and r2 score.\n",
    "        mse = mean_squared_error(testing_y, pred)\n",
    "        r2 = r2_score(testing_y, pred)\n",
    "        print(\"\\tMean Squared Error of \" + str(mse))\n",
    "        print(\"\\tR2 Score of \" + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() with 5 features: \n",
      "\tAccuracy of 0.8947368421052632\n",
      "\tRecall of 0.8409090909090909\n",
      "\tF1 of 0.8604651162790699\n",
      "\n",
      "LogisticRegression() with 5 features: \n",
      "\tAccuracy of 0.9473684210526315\n",
      "\tRecall of 0.8648648648648649\n",
      "\tF1 of 0.9142857142857143\n",
      "\n",
      "RandomForestClassifier() with 5 features: \n",
      "\tAccuracy of 0.9649122807017544\n",
      "\tRecall of 0.9459459459459459\n",
      "\tF1 of 0.9459459459459459\n",
      "\n",
      "DecisionTreeRegressor() with 5 features: \n",
      "\tMean Squared Error of 70048.36076173077\n",
      "\tR2 Score of -0.04506301141079594\n",
      "\n",
      "LinearRegression() with 5 features: \n",
      "\tMean Squared Error of 65822.867118306\n",
      "\tR2 Score of 0.10330078392720743\n",
      "\n",
      "RandomForestRegressor() with 5 features: \n",
      "\tMean Squared Error of 62593.707152789866\n",
      "\tR2 Score of 0.08811542258267602\n",
      "\n",
      "DecisionTreeClassifier() with 10 features: \n",
      "\tAccuracy of 0.8859649122807017\n",
      "\tRecall of 0.8055555555555556\n",
      "\tF1 of 0.8169014084507044\n",
      "\n",
      "LogisticRegression() with 10 features: \n",
      "\tAccuracy of 0.9035087719298246\n",
      "\tRecall of 0.7727272727272727\n",
      "\tF1 of 0.8607594936708862\n",
      "\n",
      "RandomForestClassifier() with 10 features: \n",
      "\tAccuracy of 0.9385964912280702\n",
      "\tRecall of 0.875\n",
      "\tF1 of 0.8888888888888888\n",
      "\n",
      "DecisionTreeRegressor() with 10 features: \n",
      "\tMean Squared Error of 72207.3996197402\n",
      "\tR2 Score of -0.11999744003739754\n",
      "\n",
      "LinearRegression() with 10 features: \n",
      "\tMean Squared Error of 61624.99222989268\n",
      "\tR2 Score of 0.11563983856878857\n",
      "\n",
      "RandomForestRegressor() with 10 features: \n",
      "\tMean Squared Error of 55627.65837786321\n",
      "\tR2 Score of 0.07677599177516148\n",
      "\n",
      "DecisionTreeClassifier() with 15 features: \n",
      "\tAccuracy of 0.9385964912280702\n",
      "\tRecall of 0.8936170212765957\n",
      "\tF1 of 0.9230769230769231\n",
      "\n",
      "LogisticRegression() with 15 features: \n",
      "\tAccuracy of 0.9912280701754386\n",
      "\tRecall of 1.0\n",
      "\tF1 of 0.9879518072289156\n",
      "\n",
      "RandomForestClassifier() with 15 features: \n",
      "\tAccuracy of 0.9385964912280702\n",
      "\tRecall of 0.92\n",
      "\tF1 of 0.9292929292929293\n",
      "\n",
      "DecisionTreeRegressor() with 15 features: \n",
      "\tMean Squared Error of 73065.68746143568\n",
      "\tR2 Score of -0.03535223204746418\n",
      "\n",
      "LinearRegression() with 15 features: \n",
      "\tMean Squared Error of 65220.57575108423\n",
      "\tR2 Score of 0.06536602885686493\n",
      "\n",
      "RandomForestRegressor() with 15 features: \n",
      "\tMean Squared Error of 62155.35781923065\n",
      "\tR2 Score of 0.1348175636084602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [DecisionTreeClassifier(), LogisticRegression(), RandomForestClassifier()]\n",
    "regressors = [DecisionTreeRegressor(), LinearRegression(), RandomForestRegressor()]\n",
    "for ftrs in range(5, 20, 5):\n",
    "    for clf in classifiers:\n",
    "        makeModel(ftrs, clf, cancer_df, cancer_y, True)\n",
    "        print()\n",
    "    for reg in regressors:\n",
    "        makeModel(ftrs, reg, solar_df, solar_y, False)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**\n",
    "\n",
    "CSCI 6380 with Dr. Fred Maier\n",
    "\n",
    "MIT Open Courseware\n",
    "\n",
    "[CMU Lecture Notes on Decision Trees](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
